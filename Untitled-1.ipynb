{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w_/6x800hl95xv8j0zzn6nhwgnw0000gn/T/ipykernel_89087/33740960.py:1: DtypeWarning: Columns (163) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(\"alll_combined_output.csv\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = pd.read_csv(\"alll_combined_output.csv\")\n",
    "colss=[\n",
    "    'address1', 'city', 'state', 'country', 'industry', 'sector', \n",
    "    'recommendationKey', 'fullTimeEmployees', 'overallRisk', 'payoutRatio', \n",
    "    'volume', 'averageVolume', 'averageVolume10days', 'averageDailyVolume10Day', \n",
    "    'fiftyDayAverage', 'currency', 'enterpriseValue', 'totalCashPerShare', \n",
    "    'revenueGrowth', 'operatingMargins', 'Year', 'EBITDA', 'EBIT', 'Total Expenses', \n",
    "    'Profit', 'Total Profit', 'Cash Flow', 'Employees', 'ebitda', 'ebitdaMargins', \n",
    "    'totalAssets', 'threeYearAverageReturn', 'fiveYearAverageReturn'\n",
    "]\n",
    "\n",
    "data[colss]\n",
    "data_string = data[:200].to_string(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_string = data[:20].to_string(index=False)\n",
    "# !pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please upload a CSV file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w_/6x800hl95xv8j0zzn6nhwgnw0000gn/T/ipykernel_89087/3741206394.py:26: DtypeWarning: Columns (43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(csv_path)\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import pandas as pd\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "\n",
    "# Configure Generative AI API\n",
    "genai.configure(api_key=\"AIzaSyBcwdRlhbzt257z165Ox6IS_9TRE3VaEpY\")\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "# Function to upload CSV file\n",
    "def upload_csv():\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()  # Hide the main tkinter window\n",
    "    file_path = filedialog.askopenfilename(\n",
    "        title=\"Select a CSV File\",\n",
    "        filetypes=((\"CSV Files\", \"*.csv\"), (\"All Files\", \"*.*\"))\n",
    "    )\n",
    "    return file_path\n",
    "\n",
    "# Ask user to upload a file\n",
    "print(\"Please upload a CSV file.\")\n",
    "csv_path = upload_csv()\n",
    "\n",
    "if csv_path:\n",
    "    # Read the uploaded CSV file\n",
    "    data = pd.read_csv(csv_path)\n",
    "    data_string = data[:200].to_string(index=False)\n",
    "\n",
    "    # Define the context\n",
    "    context = f\"\"\"\n",
    "    You are the Finance Head of a company. I have provided a dataset below that contains financial data, including revenue, expenses, profit margins, operational costs, and other relevant metrics. Your task is to analyze this data and provide insights and recommendations.\n",
    "\n",
    "    ### Dataset:\n",
    "    {data_string}\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate content\n",
    "    response = model.generate_content([context])\n",
    "    # print(\"\\n### AI Response:\\n\")\n",
    "    # print(response.text)\n",
    "else:\n",
    "    print(\"No file was selected.\")\n",
    "\n",
    "\n",
    "questions = [\n",
    "    \"give me top 3 companies with high profits\"\n",
    "]\n",
    "\n",
    "# Ask questions one by one\n",
    "for question in questions:\n",
    "    query = f\"{context}\\n\\n### Question: {question}\"\n",
    "    response = model.generate_content([query])\n",
    "    print(f\"### Question: {question}\")\n",
    "    print(response.text)\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Question: give me top 3 companies with high profits\n",
      "Based on the provided dataset, determining the \"top 3 companies with high profits\" requires clarification.  The dataset offers several profit-related metrics (`Net Income`, `EBIT`, `Profit`, `Total Profit`), and it's unclear which metric should be prioritized.  Also, \"high\" is relative and requires a defined threshold.\n",
      "\n",
      "To address this, I'll present the top 3 companies based on each of the profit metrics for the *most recent year available (2023)*.  This assumes that higher values within the chosen metrics represent higher profits. Note that some companies have missing data for 2023 and therefore aren't included in the rankings for those years.\n",
      "\n",
      "\n",
      "\n",
      "**Top 3 Companies by Metric (2023):**\n",
      "\n",
      "* **Net Income:**\n",
      "\n",
      "1. JPMorgan Chase & Co.: $4.9552e+10\n",
      "2. Brookfield Corporation: $1.1300e+09\n",
      "3. Value Line, Inc.: $1.8069e+07\n",
      "\n",
      "\n",
      "* **EBIT:**\n",
      "\n",
      "1. JPMorgan Chase & Co.: $1.5108e+07 (Note that this value is identical across multiple companies and years, suggesting an error in the data.)\n",
      "2. Brookfield Corporation: $2.1619e+10\n",
      "3. Triton International Limited: $6.068950e+08\n",
      "\n",
      "\n",
      "* **Profit:**\n",
      "\n",
      "1. JPMorgan Chase & Co.: $1.5108e+07 (Again, this value is problematic due to duplication.)\n",
      "2. Brookfield Corporation: $2.1619e+10\n",
      "3. Triton International Limited: $6.068950e+08\n",
      "\n",
      "\n",
      "* **Total Profit:**\n",
      "\n",
      "1. JPMorgan Chase & Co.: $4.9552e+10\n",
      "2. Brookfield Corporation: $2.2749e+10\n",
      "3. Triton International Limited: $1.080984e+09\n",
      "\n",
      "\n",
      "**Recommendations:**\n",
      "\n",
      "1. **Data Quality Check:**  The repeated identical `EBIT` and `Profit` values across numerous companies and years strongly suggest data errors.  A thorough data quality check and correction are crucial before making any further analysis.  The source of the data should be reviewed.\n",
      "\n",
      "2. **Metric Selection:** Define which profit metric is the most relevant for your company's needs.  Net Income is generally considered the most comprehensive measure of profitability, as it reflects the profit after all expenses, including taxes and interest.  However, depending on the context, EBITDA or EBIT may be more appropriate.\n",
      "\n",
      "3. **Normalization:** Consider normalizing the profit figures by revenue or assets to allow for a fairer comparison between companies of different sizes.  This will provide a more meaningful measure of profitability such as profit margin.\n",
      "\n",
      "4. **Time Series Analysis:** Analyze the trend in profits over the years (2019-2023) for better understanding of company performance and stability.  Looking at a single year might be misleading.\n",
      "\n",
      "5. **Qualitative Factors:** Financial metrics alone don't tell the whole story.  Further research into the qualitative aspects of each company's business model, competitive landscape, and management quality is needed for a complete understanding of their true profitability and long-term prospects.\n",
      "\n",
      "\n",
      "Once the data quality is addressed and a definitive profit metric is chosen, a more accurate and insightful ranking of the top 3 companies can be provided.\n",
      "\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"give me top 3 companies with high profits\"\n",
    "]\n",
    "\n",
    "# Ask questions one by one\n",
    "for question in questions:\n",
    "    query = f\"{context}\\n\\n### Question: {question}\"\n",
    "    response = model.generate_content([query])\n",
    "    print(f\"### Question: {question}\")\n",
    "    print(response.text)\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# response = model.generate_content([\"so given data is a csv file for the dataset, i want you to think as an finance head of a company and help me with the questions\", data_string])\n",
    "# print(response.text)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 133317 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m ask_gpt(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhich company has the highest sales\u001b[39m\u001b[38;5;124m\"\u001b[39m, data_string)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m, in \u001b[0;36mask_gpt\u001b[0;34m(question, data_string)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mask_gpt\u001b[39m(question, data_string):\n\u001b[1;32m      9\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a master in finances, your work is to help me with the data, Here is some data on finances of companies:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mdata_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBased on this data, help me solve my isses answer the following question:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 12\u001b[0m     chat_completion \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     13\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \n\u001b[1;32m     14\u001b[0m         messages\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     15\u001b[0m             {\n\u001b[1;32m     16\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt,\n\u001b[1;32m     18\u001b[0m             }\n\u001b[1;32m     19\u001b[0m         ]\n\u001b[1;32m     20\u001b[0m     )\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chat_completion[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/openai/_utils/_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py:829\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    826\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    827\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    828\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m    830\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    831\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[1;32m    832\u001b[0m             {\n\u001b[1;32m    833\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m    834\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m    835\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[1;32m    836\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m    837\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m    838\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m    839\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m    840\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m    841\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[1;32m    842\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m    843\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    844\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[1;32m    845\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m    846\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[1;32m    847\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: prediction,\n\u001b[1;32m    848\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m    849\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m    850\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m    851\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[1;32m    852\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m    853\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[1;32m    854\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m    855\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[1;32m    856\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m    857\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m    858\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m    859\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m    860\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m    861\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m    862\u001b[0m             },\n\u001b[1;32m    863\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m    864\u001b[0m         ),\n\u001b[1;32m    865\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m    866\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    867\u001b[0m         ),\n\u001b[1;32m    868\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m    869\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    870\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[1;32m    871\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/openai/_base_client.py:1278\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1265\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1266\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1273\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1274\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1275\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1276\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1277\u001b[0m     )\n\u001b[0;32m-> 1278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/openai/_base_client.py:955\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    953\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 955\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    956\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    957\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    958\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    959\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    960\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m    961\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/openai/_base_client.py:1059\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1056\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1058\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1059\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1062\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1063\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1067\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1068\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 133317 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}"
     ]
    }
   ],
   "source": [
    "response = ask_gpt(\"which company has the highest sales\", data_string)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w_/6x800hl95xv8j0zzn6nhwgnw0000gn/T/ipykernel_88948/1106752008.py:10: DtypeWarning: Columns (163) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv('alll_combined_output.csv')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 59\u001b[0m\n\u001b[1;32m     56\u001b[0m responses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m split_data_into_batches(data, batch_size\u001b[38;5;241m=\u001b[39mbatch_size):\n\u001b[0;32m---> 59\u001b[0m     response \u001b[38;5;241m=\u001b[39m process_batch(batch, question)\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m     61\u001b[0m         responses\u001b[38;5;241m.\u001b[39mappend(response)\n",
      "Cell \u001b[0;32mIn[8], line 39\u001b[0m, in \u001b[0;36mprocess_batch\u001b[0;34m(batch, question)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# Skip this batch or reduce its size\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Call GPT\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# try:\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     40\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     41\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     42\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt}\n\u001b[1;32m     43\u001b[0m     ]\n\u001b[1;32m     44\u001b[0m )\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Extract the content from the response\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
      "\u001b[0;31mNameError\u001b[0m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "import os\n",
    "import tiktoken\n",
    "\n",
    "# Set OpenAI API key\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")  # Replace with your API key if necessary\n",
    "\n",
    "# Load the CSV file\n",
    "data = pd.read_csv('alll_combined_output.csv')\n",
    "\n",
    "# Function to split data into batches\n",
    "def split_data_into_batches(data, batch_size=5):\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        yield data.iloc[i:i + batch_size]\n",
    "\n",
    "# Function to count tokens\n",
    "def count_tokens(text, model=\"gpt-4\"):\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "# Function to process each batch and ask GPT questions\n",
    "def process_batch(batch, question):\n",
    "    # Convert the batch to a string with fewer columns and truncated rows\n",
    "    data_string = batch.iloc[:, :3].to_string(index=False)  # Use only the first 3 columns\n",
    "    data_string = data_string[:1000]  # Truncate to 1000 characters if necessary\n",
    "    \n",
    "    # Define the prompt\n",
    "    prompt = f\"Here is some data:\\n{data_string}\\n\\nBased on this data, answer the following question:\\n{question}\"\n",
    "\n",
    "    # Ensure the token count is within the limit\n",
    "    max_tokens = 16385\n",
    "    if count_tokens(prompt) > max_tokens:\n",
    "        print(f\"Batch is too large to process.\")\n",
    "        return None  # Skip this batch or reduce its size\n",
    "\n",
    "    # Call GPT\n",
    "    # try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    # Extract the content from the response\n",
    "    return response.choices[0].message.content\n",
    "    # except openai.error.OpenAIError as e:\n",
    "    #     print(f\"An error occurred: {e}\")\n",
    "    #     return None\n",
    "\n",
    "# Question to ask\n",
    "question = \"What insights can you provide about this data?\"\n",
    "\n",
    "# Process data in batches\n",
    "batch_size = 5  # Adjust batch size to fit token limits\n",
    "responses = []\n",
    "\n",
    "for batch in split_data_into_batches(data, batch_size=batch_size):\n",
    "    response = process_batch(batch, question)\n",
    "    if response:\n",
    "        responses.append(response)\n",
    "\n",
    "# Print all responses\n",
    "for i, response in enumerate(responses):\n",
    "    print(f\"Response for Batch {i + 1}:\\n{response}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Using cached tiktoken-0.8.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/lib/python3.12/site-packages (from tiktoken) (2023.10.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from tiktoken) (2.32.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
      "Using cached tiktoken-0.8.0-cp312-cp312-macosx_11_0_arm64.whl (982 kB)\n",
      "Installing collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
